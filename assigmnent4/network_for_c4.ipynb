{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć dla agenta do gry w Connect4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39592/4128569561.py:10: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/michal/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from imp import reload\n",
    "\n",
    "import DataLoader\n",
    "reload(DataLoader)\n",
    "\n",
    "from DataLoader import InMemDataLoader\n",
    "from DataLoader import C4DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichalfica125\u001b[0m (\u001b[33mneuralnetworks\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() # klucz - 7242fe50822869937a282970b5385963778c7f8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 25, 7, 9]             225\n",
      "       BatchNorm2d-2             [-1, 25, 3, 4]              50\n",
      "            Conv2d-3             [-1, 15, 4, 5]           1,515\n",
      "       BatchNorm2d-4             [-1, 15, 2, 2]              30\n",
      "            Linear-5                   [-1, 12]             732\n",
      "       BatchNorm1d-6                   [-1, 12]              24\n",
      "           Dropout-7                   [-1, 12]               0\n",
      "            Linear-8                    [-1, 3]              39\n",
      "================================================================\n",
      "Total params: 2,615\n",
      "Trainable params: 2,615\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dp=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 25, 2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 15, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(15)\n",
    "\n",
    "        self.fc1 = nn.Linear(15*2*2, 12)\n",
    "        self.bn3 = nn.BatchNorm1d(12)\n",
    "\n",
    "        self.fc2 = nn.Linear(12, 3)\n",
    "\n",
    "        self.do = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.max_pool2d(self.conv1(x), 2)\n",
    "        x = F.relu(self.bn1(x))\n",
    "\n",
    "        x = F.max_pool2d(self.conv2(x), 2)\n",
    "        x = F.relu(self.bn2(x))\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "\n",
    "        x = self.do(x) # dropout\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = nn.Softmax()(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, Out, Targets):\n",
    "      return F.cross_entropy(Out, Targets)\n",
    "\n",
    "model = Model()\n",
    "summary(model, (2, 6, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/home/michal/Documents/studia2/2sem/neutral-networks/assigmnent4/DataLoader.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = [torch.tensor(t) for t in dataset[i]]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 57400.14it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 77446.27it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 76466.81it/s]\n"
     ]
    }
   ],
   "source": [
    "amount_of_games = 10000 \n",
    "moves_observed  = 15 \n",
    "all_samples = amount_of_games * moves_observed\n",
    "\n",
    "batch_size = 128\n",
    "train_size, val_size, test_size = int(all_samples/3), int(all_samples/3), int(all_samples/3)\n",
    "amount_of_train_batches = train_size / batch_size\n",
    "\n",
    "dataset = C4DataSet(amount_of_games, moves_observed).create_data_set()\n",
    "\n",
    "train_set = dataset[:train_size]\n",
    "val_set = dataset[train_size:train_size+val_size] \n",
    "test_set = dataset[train_size+val_size:]\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": InMemDataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "    \"valid\": InMemDataLoader(val_set, batch_size=batch_size, shuffle=False),\n",
    "    \"test\": InMemDataLoader(test_set, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(model, data_loader, device=\"cpu\"):\n",
    "  model.eval()\n",
    "\n",
    "  num_errs, num_examples = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "\n",
    "      x, y = batch[0].to(device), batch[1].to(device)\n",
    "      out = model(x)\n",
    "\n",
    "      _, pred = out.max(dim=1)\n",
    "      num_errs += (pred != y.data).sum().item()\n",
    "      num_examples += x.size(0)\n",
    "\n",
    "  return num_errs / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_of_epochs, train_loader, opt, print_every=10, device=\"cpu\"):\n",
    "  model.train()\n",
    "\n",
    "  for data_loader in data_loaders.values():\n",
    "    if isinstance(data_loader, InMemDataLoader):\n",
    "        data_loader.to(device)\n",
    "\n",
    "  min_batch_err = 100\n",
    "\n",
    "  iter = 0\n",
    "  for e in range(num_of_epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch {e+1}\")\n",
    "\n",
    "    for batch in train_loader:\n",
    "      x = batch[0].to(device)\n",
    "      y = batch[1].to(device)\n",
    "      opt.zero_grad()\n",
    "      iter += 1\n",
    "\n",
    "      out = model(x)\n",
    "      loss = nn.CrossEntropyLoss()(out, y)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      _, pred = out.max(dim=1)\n",
    "      batch_err = (pred != y).sum().item() / out.size(0)\n",
    "      min_batch_err = min(min_batch_err, batch_err)\n",
    "\n",
    "      if iter % print_every == 0:\n",
    "        print(f\"iter = {iter}, batch_err = {batch_err * 100.0}\")\n",
    "        wandb.log({\"batch_error_rate\": batch_err})\n",
    "\n",
    "    val_err = compute_error_rate(model, data_loader=data_loaders[\"valid\"], device=device)\n",
    "    print(f\"val err = {100*val_err:.2f}\")\n",
    "    wandb.log({'val_err': val_err, 'epoch': e+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'conv' in name:\n",
    "                    f_in = p.shape[1]*p.shape[2]*p.shape[3]\n",
    "                    p.normal_(0, torch.sqrt(torch.tensor(2./f_in)))\n",
    "                elif 'bn' in name:\n",
    "                    p = torch.ones_like(p)\n",
    "                elif 'fc' in name:\n",
    "                    f_in = p.shape[1]\n",
    "                    p.normal_(0, torch.sqrt(torch.tensor(2./f_in)))\n",
    "                else:\n",
    "                    raise Exception('weird weight')\n",
    "\n",
    "            elif 'bias' in name:\n",
    "                p.zero_()\n",
    "            else:\n",
    "                raise Exception('weird parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00002\n",
    "weight_decay = 0.000001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "_device = \"cpu\"\n",
    "_print_every = 30\n",
    "\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = weight_decay, momentum = momentum)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/michal/Documents/studia2/2sem/neutral-networks/assigmnent4/wandb/run-20240420_183434-2q49fiz4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2q49fiz4' target=\"_blank\">run_nr 2 10 000 games</a></strong> to <a href='https://wandb.ai/neuralnetworks/Assigmnent4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neuralnetworks/Assigmnent4' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2q49fiz4' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4/runs/2q49fiz4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "torch.Size([128, 2, 6, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_nr 2 10 000 games</strong> at: <a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2q49fiz4' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4/runs/2q49fiz4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240420_183434-2q49fiz4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "initialize_weights(model)\n",
    "\n",
    "cnt +=  1\n",
    "run_name = \"run_nr \" + str(cnt) + \" 10 000 games\" \n",
    "run = wandb.init(\n",
    "    project=\"Assigmnent4\",\n",
    "    name = run_name,\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"ADAM\",\n",
    "        \"momentum\": momentum,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"batch & dropout\": True,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model, F.cross_entropy, log=\"all\")\n",
    "\n",
    "train(num_of_epochs=epochs, train_loader=data_loaders[\"train\"], opt=opt, print_every=_print_every, device=_device)\n",
    "test_error_rate = compute_error_rate(model, data_loaders[\"test\"], device=_device)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 1., 0.],\n",
      "         [0., 1., 0., 1., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [1., 0., 1., 0., 0., 0., 1., 0.]]])\n",
      "x.shape = torch.Size([2, 6, 8])\n",
      "x.shape = torch.Size([1, 2, 6, 8])\n",
      "tensor([[0.1944, 0.4376, 0.3680]])\n",
      "0.4375842809677124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39592/812249990.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(train_set[0][0])\n",
      "/home/michal/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][0])\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor(train_set[0][0])\n",
    "    print(f\"x.shape = {x.shape}\")\n",
    "    x = x[None, :, :, :]\n",
    "    print(f\"x.shape = {x.shape}\")\n",
    "    out = model(x)\n",
    "    print(out)\n",
    "    print(out[0][1].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
