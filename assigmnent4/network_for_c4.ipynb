{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć dla agenta do gry w Connect4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from imp import reload\n",
    "\n",
    "import DataLoader\n",
    "reload(DataLoader)\n",
    "\n",
    "from DataLoader import InMemDataLoader\n",
    "from DataLoader import C4DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_games = 1\n",
    "moves_observed  = \"all\"\n",
    "\n",
    "dataset = C4DataSet(nr_of_games, moves_observed).create_data_set(task_nr=2)\n",
    "\n",
    "batch_size = 128\n",
    "train_size, val_size, test_size = int(len(dataset)/3), int(len(dataset)/3), len(dataset)/3\n",
    "amount_of_train_batches = train_size / batch_size\n",
    "\n",
    "print(f\"train_size = {train_size}\")\n",
    "train_set = dataset[:train_size]\n",
    "val_set = dataset[train_size:train_size+val_size] \n",
    "test_set = dataset[train_size+val_size:]\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": InMemDataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "    \"valid\": InMemDataLoader(val_set, batch_size=batch_size, shuffle=False),\n",
    "    \"test\": InMemDataLoader(test_set, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verti 3, 4\n",
      "hole in 4, 2\n",
      "hole in 3, 1\n",
      "triple skos lewo 4, 4\n",
      "triple skos lewo 4, 5\n",
      "triple skos lewo 4, 5\n",
      "triple skos lewo 3, 4\n",
      "board =\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 2., 1., 0., 0.],\n",
      "        [0., 2., 1., 2., 2., 0., 0.],\n",
      "        [1., 1., 2., 1., 2., 2., 0.],\n",
      "        [2., 1., 1., 1., 2., 1., 0.]])\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 0., 0., 1.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 1.]), 1)\n",
      "(tensor([0., 1., 0., 1., 0., 0., 0., 1.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 0.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 0., 0., 1.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 1.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 1., 0., 3.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 2.]), 1)\n",
      "(tensor([0., 1., 0., 0., 0., 1., 0., 4.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 3.]), 1)\n",
      "(tensor([0., 1., 0., 0., 1., 0., 0., 4.]), 1)\n",
      "(tensor([0., 2., 0., 0., 0., 0., 0., 2.]), 1)\n",
      "(tensor([0., 1., 0., 0., 1., 0., 0., 3.]), 1)\n",
      "(tensor([0., 2., 0., 0., 1., 0., 0., 3.]), 1)\n",
      "(tensor([1., 1., 0., 0., 1., 0., 0., 2.]), 1)\n"
     ]
    }
   ],
   "source": [
    "import DataLoader\n",
    "reload(DataLoader)\n",
    "from DataLoader import C4DataSet\n",
    "\n",
    "# game = 'S233322625223356466550404A' #<-- dobry przykłąd skosu, dziury \n",
    "# game = \"S3256323326066002103304101B\"\n",
    "game = \"S3410241441125533221302A\"\n",
    "dataset = C4DataSet(1, \"all\")\n",
    "samples = dataset.test_samples(game)\n",
    "for sample in samples:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rozmiar zbioru treningowego = 11\n",
      "(tensor([0., 2., 0., 0.]), 0)\n",
      "(tensor([0., 1., 0., 0.]), 0)\n",
      "(tensor([0., 2., 0., 0.]), 0)\n",
      "(tensor([0., 1., 0., 0.]), 0)\n",
      "(tensor([0., 2., 1., 0.]), 0)\n",
      "(tensor([0., 1., 1., 0.]), 0)\n",
      "(tensor([0., 2., 1., 0.]), 0)\n",
      "(tensor([0., 1., 1., 0.]), 0)\n",
      "(tensor([0., 2., 1., 0.]), 0)\n",
      "(tensor([0., 1., 1., 0.]), 0)\n",
      "(tensor([1., 2., 1., 0.]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"rozmiar zbioru treningowego = {len(train_set)}\")\n",
    "\n",
    "for sample in test_set:\n",
    "    print(sample)\n",
    "\n",
    "# 33 12 11 56 55 10 35 33 65 51 20 66 02 20 23 40 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() # klucz - 7242fe50822869937a282970b5385963778c7f8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dp=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, 4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 5, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.fc1 = nn.Linear(5*5*6, 2)\n",
    "        self.bn3 = nn.BatchNorm1d(3)\n",
    "        self.do = nn.Dropout(0.45)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.do(x) # dropout\n",
    "        x = nn.Softmax()(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, Out, Targets):\n",
    "      return F.cross_entropy(Out, Targets)\n",
    "\n",
    "model = Model()\n",
    "summary(model, (2, 6, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(model, data_loader, device=\"cpu\"):\n",
    "  model.eval()\n",
    "\n",
    "  num_errs, num_examples = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "\n",
    "      x, y = batch[0].to(device), batch[1].to(device)\n",
    "      out = model(x)\n",
    "\n",
    "      _, pred = out.max(dim=1)\n",
    "      num_errs += (pred != y.data).sum().item()\n",
    "      num_examples += x.size(0)\n",
    "\n",
    "  return num_errs / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_of_epochs, train_loader, opt, print_every=10, device=\"cpu\"):\n",
    "  model.train()\n",
    "\n",
    "  for data_loader in data_loaders.values():\n",
    "    if isinstance(data_loader, InMemDataLoader):\n",
    "        data_loader.to(device)\n",
    "\n",
    "  iter = 0\n",
    "  for e in range(num_of_epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch {e+1}\")\n",
    "\n",
    "    for batch in train_loader:\n",
    "      iter += 1\n",
    "\n",
    "      x = batch[0].to(device)\n",
    "      y = batch[1].to(device)\n",
    "     \n",
    "      opt.zero_grad()\n",
    "      out = model(x)\n",
    "      loss = nn.CrossEntropyLoss()(out, y)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      _, pred = out.max(dim=1)\n",
    "      batch_err = (pred != y).sum().item() / out.size(0)\n",
    "\n",
    "      if iter % print_every == 0:\n",
    "        print(f\"iter = {iter}, batch_err = {batch_err * 100.0}\")\n",
    "        wandb.log({\"batch_error_rate\": batch_err})\n",
    "\n",
    "    val_err = compute_error_rate(model, data_loader=data_loaders[\"valid\"], device=device)\n",
    "    print(f\"val err = {100*val_err:.2f}\")\n",
    "    wandb.log({'val_err': val_err, 'epoch': e+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    with torch.no_grad():\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'conv' in name:\n",
    "                    f_in = p.shape[1]*p.shape[2]*p.shape[3]\n",
    "                    p.normal_(0, torch.sqrt(torch.tensor(2./f_in)))\n",
    "                elif 'bn' in name:\n",
    "                    p = torch.ones_like(p)\n",
    "                elif 'fc' in name:\n",
    "                    f_in = p.shape[1]\n",
    "                    p.normal_(0, torch.sqrt(torch.tensor(2./f_in)))\n",
    "                else:\n",
    "                    raise Exception('weird weight')\n",
    "\n",
    "            elif 'bias' in name:\n",
    "                p.zero_()\n",
    "            else:\n",
    "                raise Exception('weird parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "weight_decay = 0.000001\n",
    "momentum = 0.9\n",
    "epochs = 100\n",
    "_device = \"cpu\"\n",
    "_print_every = 20\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = weight_decay, momentum = momentum)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/michal/Documents/studia2/2sem/neutral-networks/assigmnent4/wandb/run-20240425_175239-2a7jkvy0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2a7jkvy0' target=\"_blank\">run_nr 2 simple cnn network (without dropouts) 100 games</a></strong> to <a href='https://wandb.ai/neuralnetworks/Assigmnent4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neuralnetworks/Assigmnent4' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2a7jkvy0' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4/runs/2a7jkvy0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "val err = 34.32\n",
      "Epoch 2\n",
      "val err = 34.23\n",
      "Epoch 3\n",
      "iter = 20, batch_err = 40.625\n",
      "val err = 34.14\n",
      "Epoch 4\n",
      "val err = 34.14\n",
      "Epoch 5\n",
      "iter = 40, batch_err = 46.875\n",
      "val err = 34.14\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val err = 34.14\n",
      "Epoch 7\n",
      "iter = 60, batch_err = 47.65625\n",
      "val err = 34.14\n",
      "Epoch 8\n",
      "val err = 34.14\n",
      "Epoch 9\n",
      "iter = 80, batch_err = 48.4375\n",
      "val err = 34.14\n",
      "Epoch 10\n",
      "val err = 34.14\n",
      "Epoch 11\n",
      "val err = 34.14\n",
      "Epoch 12\n",
      "iter = 100, batch_err = 36.71875\n",
      "val err = 34.14\n",
      "Epoch 13\n",
      "val err = 34.14\n",
      "Epoch 14\n",
      "iter = 120, batch_err = 42.96875\n",
      "val err = 34.14\n",
      "Epoch 15\n",
      "val err = 34.14\n",
      "Epoch 16\n",
      "iter = 140, batch_err = 44.53125\n",
      "val err = 34.23\n",
      "Epoch 17\n",
      "val err = 34.14\n",
      "Epoch 18\n",
      "iter = 160, batch_err = 49.21875\n",
      "val err = 34.14\n",
      "Epoch 19\n",
      "val err = 34.14\n",
      "Epoch 20\n",
      "iter = 180, batch_err = 40.74074074074074\n",
      "val err = 34.23\n",
      "Epoch 21\n",
      "val err = 34.14\n",
      "Epoch 22\n",
      "val err = 34.23\n",
      "Epoch 23\n",
      "iter = 200, batch_err = 47.65625\n",
      "val err = 34.14\n",
      "Epoch 24\n",
      "val err = 34.14\n",
      "Epoch 25\n",
      "iter = 220, batch_err = 50.0\n",
      "val err = 34.14\n",
      "Epoch 26\n",
      "val err = 34.14\n",
      "Epoch 27\n",
      "iter = 240, batch_err = 50.78125\n",
      "val err = 34.14\n",
      "Epoch 28\n",
      "val err = 34.14\n",
      "Epoch 29\n",
      "iter = 260, batch_err = 52.34375\n",
      "val err = 34.14\n",
      "Epoch 30\n",
      "val err = 34.14\n",
      "Epoch 31\n",
      "val err = 34.14\n",
      "Epoch 32\n",
      "iter = 280, batch_err = 46.875\n",
      "val err = 34.14\n",
      "Epoch 33\n",
      "val err = 34.14\n",
      "Epoch 34\n",
      "iter = 300, batch_err = 47.65625\n",
      "val err = 34.14\n",
      "Epoch 35\n",
      "val err = 34.14\n",
      "Epoch 36\n",
      "iter = 320, batch_err = 46.875\n",
      "val err = 34.14\n",
      "Epoch 37\n",
      "val err = 34.14\n",
      "Epoch 38\n",
      "iter = 340, batch_err = 54.6875\n",
      "val err = 34.14\n",
      "Epoch 39\n",
      "val err = 34.14\n",
      "Epoch 40\n",
      "iter = 360, batch_err = 55.55555555555556\n",
      "val err = 34.14\n",
      "Epoch 41\n",
      "val err = 34.14\n",
      "Epoch 42\n",
      "val err = 34.14\n",
      "Epoch 43\n",
      "iter = 380, batch_err = 46.875\n",
      "val err = 34.14\n",
      "Epoch 44\n",
      "val err = 34.14\n",
      "Epoch 45\n",
      "iter = 400, batch_err = 49.21875\n",
      "val err = 34.14\n",
      "Epoch 46\n",
      "val err = 34.14\n",
      "Epoch 47\n",
      "iter = 420, batch_err = 42.1875\n",
      "val err = 34.14\n",
      "Epoch 48\n",
      "val err = 34.14\n",
      "Epoch 49\n",
      "iter = 440, batch_err = 53.125\n",
      "val err = 34.14\n",
      "Epoch 50\n",
      "val err = 34.14\n",
      "Epoch 51\n",
      "val err = 34.14\n",
      "Epoch 52\n",
      "iter = 460, batch_err = 46.09375\n",
      "val err = 34.14\n",
      "Epoch 53\n",
      "val err = 34.14\n",
      "Epoch 54\n",
      "iter = 480, batch_err = 47.65625\n",
      "val err = 34.14\n",
      "Epoch 55\n",
      "val err = 34.14\n",
      "Epoch 56\n",
      "iter = 500, batch_err = 39.84375\n",
      "val err = 34.14\n",
      "Epoch 57\n",
      "val err = 34.14\n",
      "Epoch 58\n",
      "iter = 520, batch_err = 45.3125\n",
      "val err = 34.23\n",
      "Epoch 59\n",
      "val err = 34.14\n",
      "Epoch 60\n",
      "iter = 540, batch_err = 48.148148148148145\n",
      "val err = 34.14\n",
      "Epoch 61\n",
      "val err = 34.14\n",
      "Epoch 62\n",
      "val err = 34.14\n",
      "Epoch 63\n",
      "iter = 560, batch_err = 37.5\n",
      "val err = 34.14\n",
      "Epoch 64\n",
      "val err = 34.14\n",
      "Epoch 65\n",
      "iter = 580, batch_err = 42.1875\n",
      "val err = 34.14\n",
      "Epoch 66\n",
      "val err = 34.14\n",
      "Epoch 67\n",
      "iter = 600, batch_err = 47.65625\n",
      "val err = 34.14\n",
      "Epoch 68\n",
      "val err = 34.14\n",
      "Epoch 69\n",
      "iter = 620, batch_err = 45.3125\n",
      "val err = 34.14\n",
      "Epoch 70\n",
      "val err = 34.14\n",
      "Epoch 71\n",
      "val err = 34.23\n",
      "Epoch 72\n",
      "iter = 640, batch_err = 53.125\n",
      "val err = 34.14\n",
      "Epoch 73\n",
      "val err = 34.23\n",
      "Epoch 74\n",
      "iter = 660, batch_err = 47.65625\n",
      "val err = 34.23\n",
      "Epoch 75\n",
      "val err = 34.14\n",
      "Epoch 76\n",
      "iter = 680, batch_err = 46.875\n",
      "val err = 34.14\n",
      "Epoch 77\n",
      "val err = 34.23\n",
      "Epoch 78\n",
      "iter = 700, batch_err = 52.34375\n",
      "val err = 34.14\n",
      "Epoch 79\n",
      "val err = 34.14\n",
      "Epoch 80\n",
      "iter = 720, batch_err = 50.0\n",
      "val err = 34.14\n",
      "Epoch 81\n",
      "val err = 34.14\n",
      "Epoch 82\n",
      "val err = 34.14\n",
      "Epoch 83\n",
      "iter = 740, batch_err = 53.90625\n",
      "val err = 34.14\n",
      "Epoch 84\n",
      "val err = 34.14\n",
      "Epoch 85\n",
      "iter = 760, batch_err = 46.09375\n",
      "val err = 34.14\n",
      "Epoch 86\n",
      "val err = 34.14\n",
      "Epoch 87\n",
      "iter = 780, batch_err = 50.0\n",
      "val err = 34.14\n",
      "Epoch 88\n",
      "val err = 34.14\n",
      "Epoch 89\n",
      "iter = 800, batch_err = 39.84375\n",
      "val err = 34.14\n",
      "Epoch 90\n",
      "val err = 34.14\n",
      "Epoch 91\n",
      "val err = 34.14\n",
      "Epoch 92\n",
      "iter = 820, batch_err = 51.5625\n",
      "val err = 34.14\n",
      "Epoch 93\n",
      "val err = 34.14\n",
      "Epoch 94\n",
      "iter = 840, batch_err = 50.78125\n",
      "val err = 34.14\n",
      "Epoch 95\n",
      "val err = 34.23\n",
      "Epoch 96\n",
      "iter = 860, batch_err = 50.78125\n",
      "val err = 34.14\n",
      "Epoch 97\n",
      "val err = 34.14\n",
      "Epoch 98\n",
      "iter = 880, batch_err = 50.78125\n",
      "val err = 34.14\n",
      "Epoch 99\n",
      "val err = 34.14\n",
      "Epoch 100\n",
      "iter = 900, batch_err = 44.44444444444444\n",
      "val err = 34.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_error_rate</td><td>▃▅▅▆▁▃▄▆▅▆▆▇▅▅▅█▅▆▃▇▅▅▂▄▁▃▅▄▇▅▅▇█▅▆▂▇▆▆▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_err</td><td>█▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_error_rate</td><td>0.44444</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>val_err</td><td>0.34137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_nr 2 simple cnn network (without dropouts) 100 games</strong> at: <a href='https://wandb.ai/neuralnetworks/Assigmnent4/runs/2a7jkvy0' target=\"_blank\">https://wandb.ai/neuralnetworks/Assigmnent4/runs/2a7jkvy0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240425_175239-2a7jkvy0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "initialize_weights(model)\n",
    "\n",
    "cnt +=  1\n",
    "run_name = \"run_nr \" + str(cnt) + \" simple cnn network (without dropouts) 100 games\" \n",
    "run = wandb.init(\n",
    "    project=\"Assigmnent4\",\n",
    "    name = run_name,\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"momentum\": momentum,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"batch & dropout\": True,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model, F.cross_entropy, log=\"all\")\n",
    "\n",
    "train(num_of_epochs=epochs, train_loader=data_loaders[\"train\"], opt=opt, print_every=_print_every, device=_device)\n",
    "test_error_rate = compute_error_rate(model, data_loaders[\"test\"], device=_device)\n",
    "wandb.finish()\n",
    "\n",
    "#  TO DO LIST: (do pierwszego zadania)\n",
    "# (0) ZAKODOWAĆ W PLANSZY kto ma ruch\n",
    "# (3) dodać dropout po kążdej warstwie \n",
    "\n",
    "# a w drugi sięc bez konwolucji (dużo cech: np. czyj ruch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in data_loaders['train']:\n",
    "        x = x.to(device)\n",
    "        print(f\"x = {x.shape}\")\n",
    "        # print(f\"y = {y}\")\n",
    "        out = model(x)\n",
    "        print(f\"out = {out}\")\n",
    "        _, pred = out.max(dim=1)\n",
    "        print(f\"pred ={pred}\")\n",
    "\n",
    "        batch_err = (pred != y).sum().item() / out.size(0)\n",
    "\n",
    "        print(batch_err)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
